{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gauravhulmukh/Chatbot-Development-for-Regional-language-using-Artificial-Intelligence/blob/master/Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "jXHOdLeZ2lhz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "b5353ac8-14a3-46c0-d856-f943b2496d95"
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import json\n",
        "import random\n",
        "\n",
        "# things we need for NLP\n",
        "import nltk\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "# things we need for Tensorflow\n",
        "import numpy as np\n",
        "import tflearn\n",
        "import tensorflow as tf\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "IwA19aaU4B-1",
        "colab_type": "code",
        "outputId": "b7421e15-8286-4635-8deb-874d797a3f5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AJZ56LXL3ymr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#importing dataset\n",
        "import json\n",
        "with open('/content/drive/My Drive/Colab Notebooks/intents.json') as json_data:\n",
        "  intents = json.load(json_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jqUkzDNc5MuX",
        "colab_type": "code",
        "outputId": "d265eaf4-9043-4441-a5c1-ae72d67e5aad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "#simply creating array\n",
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_words = ['?']#it will ignore  question MARK from pattern\n",
        "\n",
        "#create loop intents->patterns\n",
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        #seperate words:hi there=hi,there\n",
        "        w = nltk.word_tokenize(pattern)\n",
        "        # add seperated words from each pattern into words array\n",
        "        words.extend(w)\n",
        "        #create set of words(tokenize_pattern) + tag =Documents\n",
        "        documents.append((w, intent['tag']))\n",
        "        # add to our classes list\n",
        "        if intent['tag'] not in classes:\n",
        "            classes.append(intent['tag'])\n",
        "\n",
        "# stem and lower each word and remove duplicates\n",
        "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
        "words = sorted(list(set(words)))\n",
        "\n",
        "# remove duplicates\n",
        "classes = sorted(list(set(classes)))\n",
        "print (len(documents),documents)\n",
        "print (len(classes), \"classes\", classes)\n",
        "print (len(words), \"unique stemmed words\", words)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22 [(['Hi'], 'greeting'), (['How', 'are', 'you'], 'greeting'), (['Is', 'anyone', 'there', '?'], 'greeting'), (['Hello'], 'greeting'), (['Good', 'day'], 'greeting'), (['Bye'], 'goodbye'), (['See', 'you', 'later'], 'goodbye'), (['Goodbye'], 'goodbye'), (['Thanks'], 'thanks'), (['Thanks', 'you'], 'thanks'), (['That', \"'s\", 'helpful'], 'thanks'), (['how', 'to', 'register', 'payment', '?'], 'paymentregistration'), (['what', 'I', 'need', 'to', 'need', 'to', 'create', 'payment', '?'], 'paymentregistration'), (['How', 'to', 'enter', 'new', 'payment', '?'], 'paymentregistration'), (['Please', 'help', 'to', 'validate', 'payment', '?'], 'paymentvalidate'), (['Is', 'payment', 'valid', '?'], 'paymentvalidate'), (['What', 'needs', 'to', 'be', 'done', 'for', 'invoice', 'payment', 'validation', '?'], 'paymentvalidate'), (['How', 'to', 'pay', 'invoice', '?'], 'startpayment'), (['I', 'would', 'to', 'submit', 'invoice', 'for', 'payment'], 'startpayment'), (['How', 'it', 'works', 'to', 'pay', '?'], 'startpayment'), (['payment'], 'payment'), (['instructions'], 'instructions')]\n",
            "8 classes ['goodbye', 'greeting', 'instructions', 'payment', 'paymentregistration', 'paymentvalidate', 'startpayment', 'thanks']\n",
            "38 unique stemmed words [\"'s\", 'anyon', 'ar', 'be', 'bye', 'cre', 'day', 'don', 'ent', 'for', 'good', 'goodby', 'hello', 'help', 'hi', 'how', 'i', 'instruct', 'invo', 'is', 'it', 'lat', 'nee', 'new', 'pay', 'pleas', 'reg', 'see', 'submit', 'thank', 'that', 'ther', 'to', 'valid', 'what', 'work', 'would', 'you']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "n6i0z-IL5Xgc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#creating arrays for training data\n",
        "training = []\n",
        "output = []\n",
        "# create an empty array for our output\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "# training set, bag of words for each sentence\n",
        "for doc in documents:\n",
        "    # initialize our bag of words\n",
        "    bag = []\n",
        "    # list of tokenized words for the pattern\n",
        "    pattern_words = doc[0]\n",
        "    # stem each word\n",
        "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
        "    # create our bag of words array\n",
        "    for w in words:\n",
        "        bag.append(1) if w in pattern_words else bag.append(0)\n",
        "        #it is comparing alphabhetical ordered array of words with non-alphabhetical ordered array of stem words of document\n",
        "       #match 1 else 0     \n",
        "        \n",
        "    # output is a '0' for each tag and '1' for current tag\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "    #it is comparing classes array & doc[1] array\n",
        "\n",
        "    training.append([bag, output_row])\n",
        "\n",
        "# shuffle our features and turn into np.array\n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "\n",
        "# create train and test lists\n",
        "train_x = list(training[:, 0])\n",
        "train_y = list(training[:, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GxpNNH-f5gbv",
        "colab_type": "code",
        "outputId": "253dae5d-7b84-4d12-c04e-a46ffb17b706",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "cell_type": "code",
      "source": [
        "#Clears the default graph stack and resets the global default graph.\n",
        "tf.reset_default_graph()\n",
        "# Build neural network\n",
        "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
        "net = tflearn.fully_connected(net, 8)\n",
        "net = tflearn.fully_connected(net, 8)\n",
        "net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n",
        "net = tflearn.regression(net)\n",
        "\n",
        "# Define model and setup tensorboard\n",
        "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n",
        "# Start training (apply gradient descent algorithm)\n",
        "model.fit(train_x, train_y, n_epoch=1000, batch_size=8, show_metric=True)\n",
        "model.save('model.tflearn')\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Step: 2999  | total loss: \u001b[1m\u001b[32m1.58223\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 1000 | loss: 1.58223 - acc: 0.8253 -- iter: 16/22\n",
            "Training Step: 3000  | total loss: \u001b[1m\u001b[32m2.04141\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 1000 | loss: 2.04141 - acc: 0.7761 -- iter: 22/22\n",
            "--\n",
            "INFO:tensorflow:/content/model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bptpbT_W51dW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pickle.dump({\n",
        "    'words': words,\n",
        "    'classes': classes,\n",
        "    'train_x': train_x,\n",
        "    'train_y': train_y\n",
        "    }, open('training_data', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}